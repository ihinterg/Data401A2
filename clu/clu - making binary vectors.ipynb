{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    punct = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\t)\")\n",
    "    text = text.lower()\n",
    "    return word_tokenize(re.sub(punct, \" \", text))\n",
    "\n",
    "def build_vocabulary(df):\n",
    "    \"\"\"\n",
    "    create a vocab + number of \n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary = set()\n",
    "    counts = {}\n",
    "    \n",
    "    for i in df[\"index\"]:\n",
    "        texttosplit = list(df[df[\"index\"] == i][\"review/text\"])[0]\n",
    "        cleaned = cleaner(texttosplit)\n",
    "        \n",
    "        for word in cleaned:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.add(word)\n",
    "                counts.update({word: 1})\n",
    "            else:\n",
    "                counts[word] += 1\n",
    "                \n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"beer.csv\")\n",
    "df = df.dropna(subset=['review/text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab, frequencies = build_vocabulary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectors(df, vocab, frequencies, vocabsize = 3000):\n",
    "    \n",
    "    vectors = {}\n",
    "    realvocab = sorted(frequencies.items(), \n",
    "                       key = lambda x: x[1], \n",
    "                       reverse = True)[:vocabsize]\n",
    "    \n",
    "    idx = {realvocab[c][0]: c for c in range(vocabsize)}\n",
    "    #print(idx)\n",
    "    \n",
    "    for identifier in df[\"index\"]:\n",
    "        \n",
    "        texttosplit = list(df[df[\"index\"] == identifier][\"review/text\"])[0]\n",
    "        cleaned = set(cleaner(texttosplit))\n",
    "        vector = []\n",
    "        \n",
    "        for word in realvocab:\n",
    "            if word[0] in cleaned:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        vectors.update({identifier: vector})\n",
    "    return realvocab, vectors\n",
    "    #for word in realvocab:\n",
    "    #    word[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "realvocab, vectors = build_vectors(df, vocab, frequencies, vocabsize = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectors = pd.DataFrame.from_dict(vectors, orient = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "allvocab = [word[0] for word in realvocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_vectors.columns = allvocab\n",
    "df_vectors[\"index\"] = df_vectors.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupcols_pos = [\n",
    "    'part_of_speech_#', 'part_of_speech_$',\n",
    "    'part_of_speech_\\'\\'', 'part_of_speech_(', 'part_of_speech_)',\n",
    "    'part_of_speech_:', 'part_of_speech_CC', 'part_of_speech_CD',\n",
    "    'part_of_speech_DT', 'part_of_speech_IN', 'part_of_speech_JJ',\n",
    "    'part_of_speech_JJR', 'part_of_speech_JJS', 'part_of_speech_LS',\n",
    "    'part_of_speech_MD', 'part_of_speech_NN', 'part_of_speech_NNS',\n",
    "    'part_of_speech_POS', 'part_of_speech_PRP', 'part_of_speech_PRP$',\n",
    "    'part_of_speech_RB', 'part_of_speech_RBR', 'part_of_speech_TO',\n",
    "    'part_of_speech_VB', 'part_of_speech_VBD', 'part_of_speech_VBG',\n",
    "    'part_of_speech_VBN', 'part_of_speech_VBP', 'part_of_speech_VBZ',\n",
    "    'part_of_speech_WDT', 'part_of_speech_WP', 'part_of_speech_WP$',\n",
    "    'part_of_speech_WRB', 'part_of_speech_``'\n",
    "    \n",
    "]\n",
    "\n",
    "groupcols = ['sentiment_anger', 'sentiment_anticipation',\n",
    "             'sentiment_disgust', 'sentiment_fear', 'sentiment_joy',\n",
    "             'sentiment_negative', 'sentiment_positive', 'sentiment_sadness',\n",
    "             'sentiment_surprise', 'sentiment_trust']\n",
    "\n",
    "indepcols = ['beer/ABV',\n",
    "             'stopwordcount', 'charcount', 'cursewordcount', 'uniquecursewordcount',\n",
    "             'exceptional_y', 'excellent_y', 'fantastic_y', 'wonderful_y', 'highly_y', 'ok_y',\n",
    "             'average_y', 'bad_y', 'corn_y', 'sipper_y', 'group_lda',\n",
    "            ]\n",
    "\n",
    "bad = ['exceptional', 'excellent', 'fantastic', 'wonderful', 'highly', 'ok',\n",
    "             'average', 'bad', 'corn', 'sipper']\n",
    "\n",
    "allvocab = [x for x in allvocab if x not in bad]\n",
    "\n",
    "indepcols = allvocab + indepcols + groupcols + groupcols_pos + [\"proportion_unique\"]\n",
    "\n",
    "depcols = ['review/appearance', 'review/aroma', 'review/overall',\n",
    "           'review/palate', 'review/taste']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"independent_variables.json\", \"w\") as outfile:\n",
    "    json.dump(indepcols, outfile)\n",
    "    \n",
    "with open(\"dependent_variables.json\", \"w\") as outfile:\n",
    "    json.dump(depcols, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"all_attributes.csv\").dropna()\n",
    "finaldf = pd.merge(df_vectors, df, on = \"index\")\n",
    "finaldf.to_csv(\"for_neural_regression.csv\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
