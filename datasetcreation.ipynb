{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data400_share/beer.csv\", dtype={'review/text' : str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['review/text', 'review/overall']].dropna()\n",
    "reviews_raw = df['review/text']\n",
    "y = df['review/overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "reviews_clean = [REPLACE_NO_SPACE.sub(\"\", row.lower()) for row in reviews_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary = True)\n",
    "cv.fit(reviews_clean)\n",
    "X = cv.transform(reviews_clean)\n",
    "X_test = cv.transform(reviews_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = [1 if rating > 4 else 0 for rating in y]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.75)\n",
    "\n",
    "for c in np.arange(.01, .1, .01):\n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=.04)\n",
    "final_model = lr.fit(X, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), final_model.coef_[0]\n",
    "    )\n",
    "}\n",
    "\n",
    "best_positive = sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_negative = sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "wordlist = best_positive + best_negative\n",
    "wordlist = [x[0] for x in wordlist]\n",
    "\n",
    "tfidf = TfidfVectorizer(vocabulary = wordlist).fit_transform(reviews_clean).toarray()\n",
    "tfidfsentiment = pd.DataFrame(tfidf, columns=wordlist)\n",
    "tfidfsentiments.to_csv(\"tfidfsentiment.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**** TFIDFSENTIMENT DF CREATED ABOVE ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptional = [1 if 'exceptional' in r else 0 for r in reviews_clean]\n",
    "excellent = [1 if 'excellent' in r else 0 for r in reviews_clean]\n",
    "fantastic = [1 if 'fantastic' in r else 0 for r in reviews_clean]\n",
    "wonderful = [1 if 'wonderful' in r else 0 for r in reviews_clean]\n",
    "highly = [1 if 'highly' in r else 0 for r in reviews_clean]\n",
    "\n",
    "ok = [1 if 'ok' in r else 0 for r in reviews_clean]\n",
    "average = [1 if 'average' in r else 0 for r in reviews_clean]\n",
    "bad = [1 if 'bad' in r else 0 for r in reviews_clean]\n",
    "corn = [1 if 'corn' in r else 0 for r in reviews_clean]\n",
    "sipper = [1 if 'sipper' in r else 0 for r in reviews_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = 2)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [1 if g[0] > g[1] else 0 for g in groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.DataFrame(exceptional, columns=['exceptional'])\n",
    "new['excellent'] = excellent\n",
    "new['fantastic'] = fantastic\n",
    "new['wonderful'] = wonderful\n",
    "new['highly'] = highly\n",
    "\n",
    "new['ok'] = ok\n",
    "new['average'] = average\n",
    "new['bad'] = bad\n",
    "new['corn'] = corn\n",
    "new['sipper'] = sipper\n",
    "\n",
    "new['group_lda'] = topics\n",
    "\n",
    "new.head()\n",
    "sentiments = new.copy()\n",
    "sentiments.to_csv(\"sentiments.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**** SENTIMENTS DF CREATED ABOVE ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer = pd.read_csv(\"~/data400_share/beer.csv\")\n",
    "print(beer.shape)\n",
    "beer = beer.dropna(subset=['review/appearance'])\n",
    "beer = beer.dropna(subset=['review/aroma'])\n",
    "beer = beer.dropna(subset=['review/overall'])\n",
    "beer = beer.dropna(subset=['review/palate'])\n",
    "beer = beer.dropna(subset=['review/taste'])\n",
    "beer = beer.dropna(subset=['review/palate'])\n",
    "beer = beer.dropna(subset=['review/text'])\n",
    "beer = beer.iloc[:, list(range(0,13))]\n",
    "beer[\"review/text\"] = beer[\"review/text\"].str.replace('[^\\w\\s]',' ')\n",
    "beer[\"review/text\"] = beer[\"review/text\"].str.replace('\\t', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer[\"stopwordcount\"] = 0\n",
    "beer['charcount'] = 0\n",
    "beer['cursewordcount'] = 0\n",
    "beer['uniquecursewordcount'] = 0\n",
    "cursewords = [\"anal\",\n",
    "\"anus\",\n",
    "\"arse\",\n",
    "\"ass\",\n",
    "\"ballsack\",\n",
    "\"balls\",\n",
    "\"bastard\",\n",
    "\"bitch\",\n",
    "\"biatch\",\n",
    "\"bloody\",\n",
    "\"blowjob\",\n",
    "\"bollock\",\n",
    "\"bollok\",\n",
    "\"boner\",\n",
    "\"boob\",\n",
    "\"bugger\",\n",
    "\"bum\",\n",
    "\"butt\",\n",
    "\"clitoris\",\n",
    "\"cock\",\n",
    "\"coon\",\n",
    "\"crap\",\n",
    "\"cunt\",\n",
    "\"damn\",\n",
    "\"dick\",\n",
    "\"dildo\",\n",
    "\"dyke\",\n",
    "\"fag\",\n",
    "\"feck\",\n",
    "\"fellate\",\n",
    "\"fellatio\",\n",
    "\"felching\",\n",
    "\"fuck\",\n",
    "\"fudgepacker\",\n",
    "\"flange\",\n",
    "\"goddamn\",\n",
    "\"damn\",\n",
    "\"hell\",\n",
    "\"homo\",\n",
    "\"jerk\",\n",
    "\"jizz\",\n",
    "\"knobend\",\n",
    "\"labia\",\n",
    "\"lmao\",\n",
    "\"lmfao\",\n",
    "\"muff\",\n",
    "\"nigger\",\n",
    "\"nigga\",\n",
    "\"omg\",\n",
    "\"penis\",\n",
    "\"piss\",\n",
    "\"poop\",\n",
    "\"prick\",\n",
    "\"pube\",\n",
    "\"pussy\",\n",
    "\"queer\",\n",
    "\"scrotum\",\n",
    "\"sex\",\n",
    "\"shit\",\n",
    "\"sh1t\",\n",
    "\"slut\",\n",
    "\"smegma\",\n",
    "\"spunk\",\n",
    "\"tit\",\n",
    "\"tosser\",\n",
    "\"turd\",\n",
    "\"twat\",\n",
    "\"vagina\",\n",
    "\"wank\",\n",
    "\"whore\",\n",
    "\"wtf\"]\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer[\"review/text\"] = beer[\"review/text\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(beer.shape[0]):\n",
    "    currentlist = beer.iloc[i, 11]\n",
    "    currentlist = list(map(str.lower,beer.iloc[i, 11]))\n",
    "    beer.iloc[i, 14] = (sum(len(c) for c in currentlist))\n",
    "    results1 = {}\n",
    "    results2 = {}\n",
    "    for j in stopwords:\n",
    "        results1[j] = currentlist.count(j)\n",
    "    for k in cursewords:\n",
    "        results2[k] = currentlist.count(k)\n",
    "    beer.iloc[i, 13] = sum(results1.values())\n",
    "    beer.iloc[i, 15] = sum(results2.values())\n",
    "    beer.iloc[i, 16] = sum(1 for x in results2.values() if x >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalfeatures = beer.copy()\n",
    "generalfeatures.to_csv(\"generalfeatures.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**** GENERALFEATURES DF CREATED ABOVE ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrcsentiment = pd.read_csv(\"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\", sep = \"\\t\", header = None)\n",
    "nrcsentiment.columns = [\"word\", \"sentiment\", \"indicator\"]\n",
    "nrcsentiment = nrcsentiment[nrcsentiment.indicator == 1][[\"word\", \"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"beer.csv\")\n",
    "df = df.dropna(subset=['review/text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    punct = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\t)\")\n",
    "    text = text.lower()\n",
    "    return word_tokenize(re.sub(punct, \" \", text))\n",
    " \n",
    "def unique_over_total(text):\n",
    "    numunique = len(np.unique(text))\n",
    "    return numunique / len(text)\n",
    "\n",
    "def tidy_text(df):\n",
    "    \"\"\"Takes our df and makes it into tidy data wrt text:\n",
    "    id, word\n",
    "    id, word etc...\n",
    "    \"\"\"\n",
    "    identifier = df[\"index\"]\n",
    "    words = {\"index\": [], \"word\": [], \"part_of_speech\": []}\n",
    "    uniquefeature = []\n",
    "    \n",
    "    for i in identifier:\n",
    "        texttosplit = list(df[df[\"index\"] == i][\"review/text\"])[0]\n",
    "        cleaned = cleaner(texttosplit)\n",
    "        \n",
    "        uniquefeature.append(unique_over_total(cleaned))\n",
    "        \n",
    "        for j in np.unique(cleaned):\n",
    "            words[\"index\"].append(i)\n",
    "            words[\"word\"].append(j)\n",
    "            words[\"part_of_speech\"].append(pos_tag([j])[0][1])\n",
    "        \n",
    "    untidydf = pd.DataFrame({\"index\": identifier, \"proportion_unique\": uniquefeature})\n",
    "    tidydf = pd.DataFrame({\"index\": words[\"index\"], \"word\": words[\"word\"], \"part_of_speech\": words[\"part_of_speech\"]})\n",
    "    \n",
    "    return untidydf, tidydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, tidydata = tidy_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentjoined = pd.merge(tidydata, nrcsentiment, on = \"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentjoined = pd.get_dummies(sentimentjoined, columns = [\"sentiment\"])\n",
    "groupcols = ['sentiment_anger', 'sentiment_anticipation',\n",
    "             'sentiment_disgust', 'sentiment_fear', 'sentiment_joy',\n",
    "             'sentiment_negative', 'sentiment_positive', 'sentiment_sadness',\n",
    "             'sentiment_surprise', 'sentiment_trust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsentiment = sentimentjoined.groupby([\"index\"])[groupcols].sum()\n",
    "finalsentiment[\"index\"] = finalsentiment.index\n",
    "finalsentiment.reset_index(drop = True, inplace = True)\n",
    "summedsentiments = finalsentiment[groupcols].sum(axis = 1)\n",
    "\n",
    "for i in groupcols:\n",
    "    finalsentiment[i] = finalsentiment[i] / summedsentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additionalfeatures = pd.merge(pd.merge(finalsentiment, metrics, on = \"index\"), df, on = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additionalfeatures.to_csv(\"additionalfeatures.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**** LEXICON/POS FEATURES CREATED ABOVE ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depcols = ['review/appearance', 'review/aroma', 'review/overall',\n",
    "           'review/palate', 'review/taste']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalfeatures = pd.read_csv(\"generalfeatures.csv\")[depcols + [\"index\", \"stopwordcount\", \"charcount\", \"cursewordcount\", \"uniquecursewordcount\"]]\n",
    "\n",
    "df = pd.merge(pd.merge(pd.read_csv(\"tfidfsentiment.csv\"), \n",
    "                       pd.read_csv(\"sentiments.csv\"), \n",
    "                       on = \"index\"), \n",
    "              pd.read_csv(\"additionalfeatures.csv\"), \n",
    "              on = \"index\")\n",
    "\n",
    "df = pd.merge(df, generalfeatureS, on = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"finalset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
